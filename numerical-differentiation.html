<!doctype html>
<html lang="">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Laika 0.19.0" />
    <title>Numerical Differentiation</title>
    
    
    <meta name="description" content="docs" />
    
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400|Source+Sans+Pro:300,400,600"
        rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Crimson+Pro:400" rel="stylesheet" type="text/css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" type="text/css" href="creative-scala.css" />
    <script src="main.js"></script>
    <script> /* for avoiding page load transitions */</script>
</head>

<body>
    <nav id="topbar"><p><a href="index.html">←</a></p>
<p><a href="https://creativescala.github.io/case-study-gradient-descent/index.html">Case Study: Machine Learning by Gradient Descent</a></p>
<p><a href="symbolic-differentiation.html">→</a></p></nav>
    <nav id="sidebar">
        <ul class="nav-list">
          <li class="level1"><a href="index.html">Case Study: Machine Learning by Gradient Descent</a></li>
          <li class="level1 active"><a href="#">Numerical Differentiation</a></li>
          <li class="level1"><a href="symbolic-differentiation.html">Symbolic Differentiation</a></li>
          <li class="level1"><a href="automatic-differentiation.html">Automatic Differentiation</a></li>
        </ul>
    </nav>

    <div id="content">
        <main class="content">
            <h1 id="numerical-differentiation" class="title">Numerical Differentiation</h1>
            <p>Numerical differentiation is perhaps the most obvious approach to finding a function&#39;s gradient (that is, differentiating it) if you&#39;re a programmer.</p>
            <p>The gradient is just a fancy name for the slope, and the slope is &quot;rise over run&quot;.</p>
            <p>$$ gradient = \frac{rise}{run} $$</p>
            <p>Using this we can estimate the gradient of <code>f</code> at a point <code>x</code> using the equation</p>
            <p>$$ gradient \approx \frac{f(x + h) - f(x)}{h} $$</p>
            <p>where \( h \) is a small number. In this equation \( h \) is the run, and \( f(x+h) - f(x) \) is the rise. This is the essential idea behind numerical differentiation.</p>
            <p>The example below shows how the estimate of the gradient changes as we change \( h \) (the point in blue) to be closer or further away from a given point (the point in black).</p>
            <div id="draw-numerical-differentiation-plot"></div>
            <script>Sine.drawNumericalDifferentiationPlot("draw-numerical-differentiation-plot")</script>
            
            <h2 id="implementation" class="section">Implementation</h2>
            <p>It&#39;s time to write some code. There is a <a href="https://github.com/creativescala/case-study-gradient-descent">repository</a> that accompanies this case study. You should download it. Your code goes into the <code>code</code> subdirectory. Within that subdirectory you&#39;ll find some existing code to calculate loss, create data sets, and so other useful utilities. Make sure you take a look at it before you start creating your own code.</p>
            <p>Let&#39;s implement numerical differentiation. Now you might think that you should implement a method that takes all of</p>
            <ul>
              <li>the function we&#39;re differentiating;</li>
              <li>the step <code>h</code>; and</li>
              <li>the point <code>x</code> at which we&#39;re differentiating.</li>
            </ul>
            <p>Instead, however, I want you to implement a method the with following signature:</p>
            <pre><code class="nohighlight"><span class="keyword">def</span><span> </span><span class="declaration-name">differentiate</span><span>(</span><span class="identifier">h</span><span>: </span><span class="type-name">Double</span><span>)(</span><span class="identifier">f</span><span>: </span><span class="type-name">Double</span><span> =&gt; </span><span class="type-name">Double</span><span>): </span><span class="type-name">Double</span><span> =&gt; </span><span class="type-name">Double</span></code></pre>
            <p>So the method is taking a function, and returns a function. The returned function takes in a point <code>x</code> and returns an approximation of the gradient at that point.</p>
            <p>This is how differentiation is treated in mathematics: the derivative of a function is itself a function. If you&#39;ve studied calculus, this is what the \(\frac{d}{dx}\) operator is doing.</p>
            <p>There is a code stub for you to work with in the file <code>NumericalDifferentiation.scala</code> within the <code>numerical</code> subdirectory. All your code should go within the <code>numerical</code> subdirectory. There are also a few tests you can use to check your implementation. You may want to add more tests.</p>
            
            <h3 id="calculating-the-gradient-of-the-loss-function" class="section">Calculating the Gradient of the Loss Function</h3>
            <p>Now we have a way to calculate the gradient of a function, we can calculate the gradient of the loss function. However, we need to get the loss function into a form that we can use with our <code>differentiate</code> method. In other words, we need to turn <code>loss</code>, a function of 3 parameters, into a function of a single parameter. (That single parameter would be <code>a</code>, as that&#39;s what we can vary to reduce the loss and hence what we want to calculate the derivate with respect to.)</p>
            <p>The secret to doing this is called <em>currying</em>, which is not the delicious dish you probably think of when you hear the word, but instead the idea that a function of two parameters can become a function of a single parameter returning another function of a single parameter. For example, if we have the function</p>
            <pre><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">sum</span><span>: (</span><span class="type-name">Double</span><span>, </span><span class="type-name">Double</span><span>) =&gt; </span><span class="type-name">Double</span><span> = (</span><span class="identifier">x</span><span>, </span><span class="identifier">y</span><span>) =&gt; </span><span class="identifier">x</span><span> + </span><span class="identifier">y</span></code></pre>
            <p>we can curry it to obtain</p>
            <pre><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">curriedSum</span><span>: </span><span class="type-name">Double</span><span> =&gt; </span><span class="type-name">Double</span><span> =&gt; </span><span class="type-name">Double</span><span> = </span><span class="identifier">x</span><span> =&gt; </span><span class="identifier">y</span><span> =&gt; </span><span class="identifier">x</span><span> + </span><span class="identifier">y</span></code></pre>
            <p>We can repeat this process for functions of any number of arguments.</p>
            <p>This is especially useful when parameter vary at different rates. For example, the data we&#39;re using doesn&#39;t change at any time, whereas we&#39;re constantly adjusting the parameter <code>a</code> to try to find the best value. I don&#39;t want to write any more as this is the main challenge here: implement a curried form of <code>loss</code> so that we can use it with <code>differentiate</code>.</p>
            <p>There is a stub in <code>Loss.scala</code> that you can work with.</p>
            
            <h3 id="gradient-descent" class="section">Gradient Descent</h3>
            <p>Now we&#39;re ready to put it all together and implement gradient descent. We&#39;re going to start by implementing a method that performs a single iteration of gradient descent. This method will take in the current value of the parameter we&#39;re optimizing, and the function we&#39;re optimizing, and return an updated value for the parameter. See the <code>iterate</code> method on <code>GradientDescent.scala</code>.</p>
            <p>In the body of the method we want to:</p>
            <ol class="arabic">
              <li>Calculate the gradient of the function at the value of the parameter, using the numerical differentiation method we have already implemented.</li>
              <li>Calculate the updated value of the parameter.</li>
            </ol>
            <p>How do we calculate the updated value? We want to adjust the parameter to reduce the loss, which means moving it a little bit in the direction of <em>negative</em> gradient. So if \( x \) is the current value of the parameter we want</p>
            <p>$$ updatedX = x - (r \times gradient) $$</p>
            <p>where \( r \) is a small value (such as 0.01) known as the <em>learning rate</em>.</p>
            <p>Go ahead and implement this!</p>
            <p>Now we can implement the full gradient descent algorithm, in the method <code>gradientDescent</code>. The only thing you need to do here is iterate for the given number of iterations. (There are other stopping conditions we can use, such as stopping when the loss doesn&#39;t decrease from one iteration to the next, but we&#39;re keeping things simple here.)</p>
            
            <h3 id="animate-it" class="section">Animate It!</h3>
            <p>Finally, complete the implementation in <code>Animation.scala</code> and draw an animation showing gradient descent at work!</p>
            <p class="nextPage"><a href="symbolic-differentiation.html">Symbolic Differentiation→</a></p>
        </main>
        <footer>Creative Scala is copyright Noel Welsh</footer>
    </div>

    </div>
</body>

</html>
