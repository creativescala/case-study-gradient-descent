<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Laika 0.18.1 + Helium Theme" />
    <title>Case Study: Machine Learning by Gradient Descent</title>
    
    <meta name="author" content="Noel Welsh"/>
    
    
    <meta name="description" content="docs"/>
    
    
    <link rel="icon" sizes="32x32" type="image/png" href="https://typelevel.org/img/favicon.png"/>
    
    
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/tonsky/FiraCode@1.207/distr/fira_code.css">
    
    <link rel="stylesheet" type="text/css" href="helium/icofont.min.css" />
    <link rel="stylesheet" type="text/css" href="helium/laika-helium.css" />
    <link rel="stylesheet" type="text/css" href="site/styles.css" />
    <script src="helium/laika-helium.js"></script>
    <script src="main.js"></script>
    <script src="main.js"></script>
    <script src="helium/laika-helium.js"></script>
    
    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script> /* for avoiding page load transitions */ </script>
  </head>

  <body>

    <header id="top-bar">

      <div class="row">
        <a id="nav-icon">
          <i class="icofont-laika" title="Navigation">&#xefa2;</i>
        </a>
        
      </div>

      <a class="icon-link" href="https://creativescala.org"><i class="icofont-laika" title="Home">&#xef47;</i></a>

      <span class="row links"><a class="icon-link svg-link" href="https://github.com/creativescala/case-study-gradient-descent"><span title="Source Code"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M49.995,1c-27.609,-0 -49.995,22.386 -49.995,50.002c-0,22.09 14.325,40.83 34.194,47.444c2.501,0.458 3.413,-1.086 3.413,-2.412c0,-1.185 -0.043,-4.331 -0.067,-8.503c-13.908,3.021 -16.843,-6.704 -16.843,-6.704c-2.274,-5.773 -5.552,-7.311 -5.552,-7.311c-4.54,-3.103 0.344,-3.042 0.344,-3.042c5.018,0.356 7.658,5.154 7.658,5.154c4.46,7.64 11.704,5.433 14.552,4.156c0.454,-3.232 1.744,-5.436 3.174,-6.685c-11.102,-1.262 -22.775,-5.553 -22.775,-24.713c-0,-5.457 1.949,-9.92 5.147,-13.416c-0.516,-1.265 -2.231,-6.348 0.488,-13.233c0,0 4.199,-1.344 13.751,5.126c3.988,-1.108 8.266,-1.663 12.518,-1.682c4.245,0.019 8.523,0.574 12.517,1.682c9.546,-6.47 13.736,-5.126 13.736,-5.126c2.728,6.885 1.013,11.968 0.497,13.233c3.204,3.496 5.141,7.959 5.141,13.416c0,19.209 -11.691,23.436 -22.83,24.673c1.795,1.544 3.394,4.595 3.394,9.26c0,6.682 -0.061,12.076 -0.061,13.715c0,1.338 0.899,2.894 3.438,2.406c19.853,-6.627 34.166,-25.354 34.166,-47.438c-0,-27.616 -22.389,-50.002 -50.005,-50.002"/>
  </g>
</svg></span></a></span>

    </header>

    <nav id="sidebar">

      <div class="row">
        <a class="icon-link svg-link" href="https://github.com/creativescala/case-study-gradient-descent"><span title="Source Code"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M49.995,1c-27.609,-0 -49.995,22.386 -49.995,50.002c-0,22.09 14.325,40.83 34.194,47.444c2.501,0.458 3.413,-1.086 3.413,-2.412c0,-1.185 -0.043,-4.331 -0.067,-8.503c-13.908,3.021 -16.843,-6.704 -16.843,-6.704c-2.274,-5.773 -5.552,-7.311 -5.552,-7.311c-4.54,-3.103 0.344,-3.042 0.344,-3.042c5.018,0.356 7.658,5.154 7.658,5.154c4.46,7.64 11.704,5.433 14.552,4.156c0.454,-3.232 1.744,-5.436 3.174,-6.685c-11.102,-1.262 -22.775,-5.553 -22.775,-24.713c-0,-5.457 1.949,-9.92 5.147,-13.416c-0.516,-1.265 -2.231,-6.348 0.488,-13.233c0,0 4.199,-1.344 13.751,5.126c3.988,-1.108 8.266,-1.663 12.518,-1.682c4.245,0.019 8.523,0.574 12.517,1.682c9.546,-6.47 13.736,-5.126 13.736,-5.126c2.728,6.885 1.013,11.968 0.497,13.233c3.204,3.496 5.141,7.959 5.141,13.416c0,19.209 -11.691,23.436 -22.83,24.673c1.795,1.544 3.394,4.595 3.394,9.26c0,6.682 -0.061,12.076 -0.061,13.715c0,1.338 0.899,2.894 3.438,2.406c19.853,-6.627 34.166,-25.354 34.166,-47.438c-0,-27.616 -22.389,-50.002 -50.005,-50.002"/>
  </g>
</svg></span></a>
      </div>

      <ul class="nav-list">
        <li class="level1 active"><a href="#">Case Study: Machine Learning by Gradient Descent</a></li>
        <li class="level1"><a href="numerical-differentation.html">Numerical Differentiation</a></li>
      </ul>

      <ul class="nav-list">
        <li class="level1 nav-header">Related Projects</li>
        
          <li class="level2"><a href="https://creativescala.org/doodle">Doodle</a></li>
        
          <li class="level2"><a href="https://creativescala.github.io/doodle-svg">Doodle SVG</a></li>
        
          <li class="level2"><a href="https://creativescala.org">Creative Scala</a></li>
        
      </ul>

    </nav>

    <div id="container">

      <nav id="page-nav">
        <p class="header"><a href="#">Case Study: Machine Learning by Gradient Descent</a></p>

        <ul class="nav-list">
          <li class="level1"><a href="#introduction">Introduction</a></li>
          <li class="level1"><a href="#gradient-descent-for-function-fitting">Gradient Descent for Function Fitting</a></li>
        </ul>

        <p class="footer"></p>
      </nav>

      <main class="content">

        <h1 id="case-study-machine-learning-by-gradient-descent" class="title">Case Study: Machine Learning by Gradient Descent</h1>
        
        <h2 id="introduction" class="section">Introduction<a class="anchor-link right" href="#introduction"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>This case study looks at gradient descent, and the application of gradient descent to machine learning. We look at gradient descent from a programming, rather than mathematical, perspective. We&#39;ll start with a simple example that describes the problem we&#39;re trying to solve and how gradient descent can be used to solve it. We&#39;ll then look at three methods to compute gradients, the core of the problem:</p>
        <ul>
          <li>numerical differentiation;</li>
          <li>symbolic differentiation; and</li>
          <li>automatic differentiation.</li>
        </ul>
        
        <h2 id="gradient-descent-for-function-fitting" class="section">Gradient Descent for Function Fitting<a class="anchor-link right" href="#gradient-descent-for-function-fitting"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>At the time of writing (September 2022), <a href="https://stablediffusionweb.com/">Stable Diffusion</a> is one of the newest, and best, text-to-image programs. Give it a try! Enter some text and see what image yous can get it to produce. It&#39;s certainly impressive, though the results are still sometimes a little bit odd.</p>
        <p>At it&#39;s core, Stable Diffusion and similar programs such as Midjourney, are functions. Remember the core idea of a function is that you put something and get something back. In this case you put in text and get back an image. What makes these functions particularly interesting is that parts of the function are learned from data. The data consists of example of text and images associated with them. The general shape of the function is fixed but many parts of it, called weights, are adjusted so that, given input, the output becomes closer to that in data used for learning.</p>
        <p>An example will help make this clearer. Consider the function below.</p>
        <p>$$f(x, a) = a \sin(x)$$</p>
        <p>In Scala we&#39;d write</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">f</span><span>: (</span><span class="type-name">Double</span><span>, </span><span class="type-name">Double</span><span>) =&gt; </span><span class="type-name">Double</span><span> = (</span><span class="identifier">x</span><span>, </span><span class="identifier">a</span><span>) =&gt; </span><span class="identifier">a</span><span> * </span><span class="type-name">Math</span><span>.</span><span class="identifier">sin</span><span>(</span><span class="identifier">x</span><span>)</span></code></pre>
        <p>This is a function with two parameters:</p>
        <ol class="arabic">
          <li><code>x</code>, which is the usual <code>x</code> value; and</li>
          <li><code>a</code>, which is the amplitude (height) of the sine wave.</li>
        </ol>
        <p>(Note that I&#39;m defining functions so that the code is closer to the mathematics, but we could equally use a Scala method.)</p>
        <p>You can play with the demo below, to see how changing the value of <code>a</code> changes the function.</p>
        <div id="draw-basic-plot"></div>
        <script>Sine.drawBasicPlot("draw-basic-plot")</script>
        <p>Now imagine we have some data, which are pairs of <code>x</code> and <code>y</code> values. For each <code>x</code> value we have the <code>y</code> value we&#39;d like the function to produce. We can adjust the value of <code>a</code> to bring the function closer or further away from the output. To quantify how good a choice we&#39;ve made for <code>a</code>, we can look at the distance between the function output and the <code>y</code> value for each data point in our data set. We&#39;ll call this the <em>loss function</em> or just the <em>loss</em>. The demo below allows you to adjust <code>a</code> and see how the the loss changes for some randomly choosen data. You should note that you can increase and decrease the loss by changing <code>a</code>.</p>
        <div id="draw-error-plot"></div>
        <script>Sine.drawErrorPlot("draw-error-plot")</script>
        <p>So we have:</p>
        <ul>
          <li>some <em>training data</em> that gives us example inputs and outputs;</li>
          <li>a function with a parameter (<code>a</code>) that we want to learn in response to data; and</li>
          <li>a way of measuring the quality of our current choice of parameter (the loss);</li>
        </ul>
        <p>Now the final piece of the puzzle is to tell the computer how to adjust the parameter to reduce the loss. This gets us to gradient descent. The <em>gradient</em> of a function is just a fancy word for the function&#39;s slope. So what we need to do is calculate the gradient of the loss function with respect to <code>a</code>, meaning:</p>
        <ol class="arabic">
          <li>find how changes in <code>a</code> relate to changes in loss, and</li>
          <li>move <code>a</code> in the direction that reduces the loss.</li>
        </ol>
        <p>We use the term <em>differentiation</em> for finding the gradient of a function.</p>
        <p>To really formalize this we need to be a bit more precise about about the error function. For one particular data point, the loss is</p>
        <p>$$ pointLoss(f, a, x, y) = (f(x, a) - y)^2 $$</p>
        <p>In Scala, we&#39;d write</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">pointLoss</span><span>: ((</span><span class="type-name">Double</span><span>, </span><span class="type-name">Double</span><span>) =&gt; </span><span class="type-name">Double</span><span>, </span><span class="type-name">Double</span><span>, </span><span class="type-name">Double</span><span>, </span><span class="type-name">Double</span><span>) =&gt; </span><span class="type-name">Double</span><span> = 
  (</span><span class="identifier">f</span><span>, </span><span class="identifier">a</span><span>, </span><span class="identifier">x</span><span>, </span><span class="identifier">y</span><span>) =&gt; {
    </span><span class="keyword">val</span><span> </span><span class="identifier">error</span><span> = </span><span class="identifier">f</span><span>(</span><span class="identifier">x</span><span>, </span><span class="identifier">a</span><span>) - </span><span class="identifier">y</span><span>
    </span><span class="identifier">error</span><span> * </span><span class="identifier">error</span><span>
  }</span></code></pre>
        <p>This means the loss for a single point is always non-negative.</p>
        <p>Now we just have to sum up the loss over all the data points to get what we commonly call the loss.</p>
        <p>$$ loss(data, f, a) = \sum_{pt \in data}pointLoss(f, a, pt.x, pt.y)$$</p>
        <p>You might not be familiar with this mathematical notation. It&#39;s not important. The Scala code is:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">loss</span><span>: (</span><span class="type-name">List</span><span>[</span><span class="type-name">Point</span><span>], (</span><span class="type-name">Double</span><span>, </span><span class="type-name">Double</span><span>) =&gt; </span><span class="type-name">Double</span><span>, </span><span class="type-name">Double</span><span>) =&gt; </span><span class="type-name">Double</span><span> =
  (</span><span class="identifier">data</span><span>, </span><span class="identifier">f</span><span>, </span><span class="identifier">a</span><span>) =&gt; 
    </span><span class="identifier">data</span><span>.</span><span class="identifier">foldLeft</span><span>(</span><span class="number-literal">0.0</span><span>){ (</span><span class="identifier">accum</span><span>, </span><span class="identifier">pt</span><span>) =&gt; </span><span class="identifier">pointLoss</span><span>(</span><span class="identifier">f</span><span>, </span><span class="identifier">a</span><span>, </span><span class="identifier">pt</span><span>.</span><span class="identifier">x</span><span>, </span><span class="identifier">pt</span><span>.</span><span class="identifier">y</span><span>) }</span></code></pre>
        <p>Because we&#39;re taking the sum of the point loss, the loss is also always non-negative. This means finding the smallest loss is always the best choice.</p>
        <p>To recap, our goal is to make the loss as small as possible. In technical jargon we&#39;d say we&#39;re minimizing the loss function. We&#39;re going to do this by calculating the gradient of the loss function with respect to <code>a</code>, and then move <code>a</code> a small amount in the direction that reduces the loss. We then repeat this process, until we can&#39;t reduce the loss any more or we get bored.</p>
        <p>Notice at this point I&#39;m not giving details. As you&#39;ve probably guessed, you&#39;re going to implement this and our first approach will be numerical differentiation.</p>

        <hr style="margin-top: 30px"/>
        <footer style="font-size: 90%; text-align: center">
          Doodle Explore is a <a href="https://creativescala.org/">Creative Scala</a> project.</footer>

      </main>

    </div>
  </body>
</html>
