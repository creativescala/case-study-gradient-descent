<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Laika 0.18.1 + Helium Theme" />
    <title>Numerical Differentiation</title>
    
    <meta name="author" content="Noel Welsh"/>
    
    
    <meta name="description" content="docs"/>
    
    
    <link rel="icon" sizes="32x32" type="image/png" href="https://typelevel.org/img/favicon.png"/>
    
    
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/tonsky/FiraCode@1.207/distr/fira_code.css">
    
    <link rel="stylesheet" type="text/css" href="helium/icofont.min.css" />
    <link rel="stylesheet" type="text/css" href="helium/laika-helium.css" />
    <link rel="stylesheet" type="text/css" href="site/styles.css" />
    <script src="helium/laika-helium.js"></script>
    <script src="main.js"></script>
    <script src="main.js"></script>
    <script src="helium/laika-helium.js"></script>
    
    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script> /* for avoiding page load transitions */ </script>
  </head>

  <body>

    <header id="top-bar">

      <div class="row">
        <a id="nav-icon">
          <i class="icofont-laika" title="Navigation">&#xefa2;</i>
        </a>
        
      </div>

      <a class="icon-link" href="https://creativescala.org"><i class="icofont-laika" title="Home">&#xef47;</i></a>

      <span class="row links"><a class="icon-link svg-link" href="https://github.com/creativescala/case-study-gradient-descent"><span title="Source Code"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M49.995,1c-27.609,-0 -49.995,22.386 -49.995,50.002c-0,22.09 14.325,40.83 34.194,47.444c2.501,0.458 3.413,-1.086 3.413,-2.412c0,-1.185 -0.043,-4.331 -0.067,-8.503c-13.908,3.021 -16.843,-6.704 -16.843,-6.704c-2.274,-5.773 -5.552,-7.311 -5.552,-7.311c-4.54,-3.103 0.344,-3.042 0.344,-3.042c5.018,0.356 7.658,5.154 7.658,5.154c4.46,7.64 11.704,5.433 14.552,4.156c0.454,-3.232 1.744,-5.436 3.174,-6.685c-11.102,-1.262 -22.775,-5.553 -22.775,-24.713c-0,-5.457 1.949,-9.92 5.147,-13.416c-0.516,-1.265 -2.231,-6.348 0.488,-13.233c0,0 4.199,-1.344 13.751,5.126c3.988,-1.108 8.266,-1.663 12.518,-1.682c4.245,0.019 8.523,0.574 12.517,1.682c9.546,-6.47 13.736,-5.126 13.736,-5.126c2.728,6.885 1.013,11.968 0.497,13.233c3.204,3.496 5.141,7.959 5.141,13.416c0,19.209 -11.691,23.436 -22.83,24.673c1.795,1.544 3.394,4.595 3.394,9.26c0,6.682 -0.061,12.076 -0.061,13.715c0,1.338 0.899,2.894 3.438,2.406c19.853,-6.627 34.166,-25.354 34.166,-47.438c-0,-27.616 -22.389,-50.002 -50.005,-50.002"/>
  </g>
</svg></span></a></span>

    </header>

    <nav id="sidebar">

      <div class="row">
        <a class="icon-link svg-link" href="https://github.com/creativescala/case-study-gradient-descent"><span title="Source Code"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M49.995,1c-27.609,-0 -49.995,22.386 -49.995,50.002c-0,22.09 14.325,40.83 34.194,47.444c2.501,0.458 3.413,-1.086 3.413,-2.412c0,-1.185 -0.043,-4.331 -0.067,-8.503c-13.908,3.021 -16.843,-6.704 -16.843,-6.704c-2.274,-5.773 -5.552,-7.311 -5.552,-7.311c-4.54,-3.103 0.344,-3.042 0.344,-3.042c5.018,0.356 7.658,5.154 7.658,5.154c4.46,7.64 11.704,5.433 14.552,4.156c0.454,-3.232 1.744,-5.436 3.174,-6.685c-11.102,-1.262 -22.775,-5.553 -22.775,-24.713c-0,-5.457 1.949,-9.92 5.147,-13.416c-0.516,-1.265 -2.231,-6.348 0.488,-13.233c0,0 4.199,-1.344 13.751,5.126c3.988,-1.108 8.266,-1.663 12.518,-1.682c4.245,0.019 8.523,0.574 12.517,1.682c9.546,-6.47 13.736,-5.126 13.736,-5.126c2.728,6.885 1.013,11.968 0.497,13.233c3.204,3.496 5.141,7.959 5.141,13.416c0,19.209 -11.691,23.436 -22.83,24.673c1.795,1.544 3.394,4.595 3.394,9.26c0,6.682 -0.061,12.076 -0.061,13.715c0,1.338 0.899,2.894 3.438,2.406c19.853,-6.627 34.166,-25.354 34.166,-47.438c-0,-27.616 -22.389,-50.002 -50.005,-50.002"/>
  </g>
</svg></span></a>
      </div>

      <ul class="nav-list">
        <li class="level1"><a href="index.html">Case Study: Machine Learning by Gradient Descent</a></li>
        <li class="level1 active"><a href="#">Numerical Differentiation</a></li>
      </ul>

      <ul class="nav-list">
        <li class="level1 nav-header">Related Projects</li>
        
          <li class="level2"><a href="https://creativescala.org/doodle">Doodle</a></li>
        
          <li class="level2"><a href="https://creativescala.github.io/doodle-svg">Doodle SVG</a></li>
        
          <li class="level2"><a href="https://creativescala.org">Creative Scala</a></li>
        
      </ul>

    </nav>

    <div id="container">

      <nav id="page-nav">
        <p class="header"><a href="#">Numerical Differentiation</a></p>

        <ul class="nav-list">
          <li class="level1"><a href="#calculating-the-gradient-of-the-loss-function">Calculating the Gradient of the Loss Function</a></li>
          <li class="level1"><a href="#gradient-descent">Gradient Descent</a></li>
        </ul>

        <p class="footer"></p>
      </nav>

      <main class="content">

        <h1 id="numerical-differentiation" class="title">Numerical Differentiation</h1>
        <p>Numerical differentiation is perhaps the most obvious approach to finding a function&#39;s gradient (that is, differentiating it) if you&#39;re a programmer.</p>
        <p>The gradient is just a fancy name for the slope, and the slope is &quot;rise over run&quot;.</p>
        <p>$$ gradient = \frac{rise}{run} $$</p>
        <p>We can estimate the gradient of <code>f</code> at a point <code>x</code> using the equation</p>
        <p>$$ gradient \approx \frac{f(x + h) - f(x)}{h} $$</p>
        <p>where <code>h</code> is a small number. This is the essential idea behind numerical differentiation.</p>
        <p>It&#39;s time to write some code. Let&#39;s implement numerical differentiation. Now you might think that you should implement a method that takes all of</p>
        <ul>
          <li>the function we&#39;re differentiating;</li>
          <li>the step <code>h</code>; and</li>
          <li>the point <code>x</code> at which we&#39;re differentiating.</li>
        </ul>
        <p>Instead, however, I want you to implement a method with following signature:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">def</span><span> </span><span class="declaration-name">numericalDifferentiation</span><span>(</span><span class="identifier">h</span><span>: </span><span class="type-name">Double</span><span>)(</span><span class="identifier">f</span><span>: </span><span class="type-name">Double</span><span> =&gt; </span><span class="type-name">Double</span><span>): </span><span class="type-name">Double</span><span> =&gt; </span><span class="type-name">Double</span></code></pre>
        <p>So the method is taking a function, and returns a function. The returned function takes in a point <code>x</code> and returns an approximation of the gradient at that point.</p>
        <p>This is how differentiation is treated in mathematics: the derivative of a function is itself a function. If you&#39;ve studied calculus, this is what the (\frac{d}{dx}) operator is doing.</p>
        
        <h2 id="calculating-the-gradient-of-the-loss-function" class="section">Calculating the Gradient of the Loss Function<a class="anchor-link right" href="#calculating-the-gradient-of-the-loss-function"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>Now we have a way to calculate the gradient of a function, we can calculate the gradient of the loss function. However, we need to get the loss function into a form that we can use with our <code>numerical Differentiation</code> method. In other words, we need to turn <code>loss</code>, a function of 3 parameters, into a function of a single parameter. (That single parameter would be <code>a</code>, as that&#39;s what we  can vary to reduce the loss and hence what we want to calculate the derivate with respect to.)</p>
        <p>The secret to doing this is called <em>currying</em>, which is not the delicious dish you probably think of when you hear the word, but instead the idea that a function of two parameters can become a function of a single parameter returning another function of a single parameter. For example, if we have the function</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">sum</span><span>: (</span><span class="type-name">Double</span><span>, </span><span class="type-name">Double</span><span>) =&gt; </span><span class="type-name">Double</span><span> = (</span><span class="identifier">x</span><span>, </span><span class="identifier">y</span><span>) =&gt; </span><span class="identifier">x</span><span> + </span><span class="identifier">y</span></code></pre>
        <p>we can curry it to obtain</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">curriedSum</span><span>: </span><span class="type-name">Double</span><span> =&gt; </span><span class="type-name">Double</span><span> =&gt; </span><span class="type-name">Double</span><span> = </span><span class="identifier">x</span><span> =&gt; </span><span class="identifier">y</span><span> =&gt; </span><span class="identifier">x</span><span> + </span><span class="identifier">y</span></code></pre>
        <p>This is especially useful when parameter vary at different rates. For example, the data we&#39;re using doesn&#39;t change at any time, whereas we&#39;ve constantly the parameter <code>a</code> to try to find the best value. I don&#39;t want to write any more as this is the main challenge here: implement a curried form of <code>loss</code> so that we can use it with <code>numericalDifferentation</code>.</p>
        
        <h2 id="gradient-descent" class="section">Gradient Descent<a class="anchor-link right" href="#gradient-descent"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>Now we&#39;re ready to put it all together. Here&#39;s a sketch of how gradient descent works:</p>
        <ul>
          <li>Start by choosing an initial value for the parameter <code>a</code>.</li>
          <li>Calculate the gradient of the loss given <code>a</code>. </li>
          <li>Move <code>a</code> a small amount (the so called <em>step size</em>) in the direction that decreases the gradient.</li>
          <li>Repeat this process until the loss is very small, the loss is not decreasing quickly, or the process has gone on long enough.</li>
        </ul>
        <p>I&#39;m a bit vague on the details above. You can choose reasonable value for the step size and other values I have omitted, and thinking about what is reasonable will force you to get a better understanding of what&#39;s going on.</p>
        <p>Implement gradient descent, and show that you can find a good value for <code>a</code> by using it.</p>

        <hr style="margin-top: 30px"/>
        <footer style="font-size: 90%; text-align: center">
          Doodle Explore is a <a href="https://creativescala.org/">Creative Scala</a> project.</footer>

      </main>

    </div>
  </body>
</html>
