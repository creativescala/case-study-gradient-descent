<!doctype html>
<html lang="">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Laika 0.19.0" />
    <title>Automatic Differentiation</title>
    
    
    <meta name="description" content="docs" />
    
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400|Source+Sans+Pro:300,400,600"
        rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Crimson+Pro:400" rel="stylesheet" type="text/css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" type="text/css" href="creative-scala.css" />
    <script src="main.js"></script>
    <script> /* for avoiding page load transitions */</script>
</head>

<body>
    <nav id="topbar"><p><a href="symbolic-differentiation.html">‚Üê</a></p>
<p><a href="https://creativescala.github.io/case-study-gradient-descent/index.html">Case Study: Machine Learning by Gradient Descent</a></p>
<p></p></nav>
    <nav id="sidebar">
        <ul class="nav-list">
          <li class="level1"><a href="index.html">Case Study: Machine Learning by Gradient Descent</a></li>
          <li class="level1"><a href="numerical-differentiation.html">Numerical Differentiation</a></li>
          <li class="level1"><a href="symbolic-differentiation.html">Symbolic Differentiation</a></li>
          <li class="level1 active"><a href="#">Automatic Differentiation</a></li>
        </ul>
    </nav>

    <div id="content">
        <main class="content">
            <h1 id="automatic-differentiation" class="title">Automatic Differentiation</h1>
            <p>We&#39;ll now look at our final method for calculating derivatives: automatic differentiation. We&#39;ll introduce it with a bit of a digression into the history of calculus.</p>
            <p>In the section on numerical differentiation we said we can approximately calculate the gradient with the following equation:</p>
            <p>$$ gradient \approx \frac{f(x + h) - f(x)}{h} $$</p>
            <p>We can work with this symbolically to calculate derivates. Let&#39;s try \(f(x) = x^2 \). We know from the section on symbolic differentiation that \(\frac{df(x)}{dx} = 2x \). Let&#39;s see what happens when we substitute \(f(x) = x^2 \) into the definition of the gradient above.</p>
            <p>$$\begin{align}
            \frac{df(x)}{dx} &amp; = \frac{f(x + h) - f(x)}{h} \\
                             &amp; = \frac{(x + h)^2 - x^2}{h} \\
                             &amp; = \frac{x^2 + 2xh + h^2 - x^2}{h} \\
                             &amp; = \frac{2xh + h^2}{h} \\
                             &amp; = \frac{h(2x + h)}{h} \\
                             &amp; = 2x + h \\
            \end{align}$$</p>
            <p>Now if we make \(h\) really really small we can just ignore it and end up with </p>
            <p>$$ \frac{df(x)}{dx} = 2x $$</p>
            <p>This is the answer we expect, but we&#39;ve done some slightly dodgy mathematics to arrive at it. The problem is with how we treat \(h\). When we said we can ignore \(h\) we&#39;re saying that \(h = 0\). However, in a prior step we had \(\frac{h}{h}\) which we said cancelled out. This works so long as \(h \neq 0\), otherwise we end up with \(\frac{0}{0}\) which is not defined. Effectively we&#39;re saying that \(h\) is infinitesimally small: so small we can ignore it when we want but still not zero so that \(\frac{h}{h}\) is defined.</p>
            <p>This type of algebra was used when calculus was first invented, but the issues with it led to the replacement with what is now the standard approach to precisely defining calculus. This approach is known as <em>standard analysis</em> and it uses limits instead of infinitesimals.</p>
            <p>However, around the 1960s the concept of infinitesimals were precisely defined, leading to a new way of defining calculus. This is known as <a href="https://en.wikipedia.org/wiki/Nonstandard_analysis">nonstandard analysis</a>.</p>
            
        </main>
        <footer>Creative Scala is copyright Noel Welsh</footer>
    </div>

    </div>
</body>

</html>
